I"ì<h2 id="the-scene-a-programming-interview">The scene: a programming interview</h2>
<p>The interview candidate is staring at me, frowning. Behind them is the whiteboard, where they have tentatively sketched out a rudimentary function prototype. They are trying to understand more about what exactly they need to do to implement the function, and itâ€™s going poorly.</p>

<p>â€œBinary data. So, you mean, zeroes and ones?â€</p>

<p>Iâ€™m fine with the question. Really the term â€œbinary dataâ€ is actually a bit nebulousâ€¦isnâ€™t all data processed by the computer ultimately â€˜binaryâ€™? So I try to clarify:</p>

<p>â€œSure, ultimately we can think of the data as the â€˜zeroes and onesâ€™ that the computer needs to work with. When I say binary data I just mean the actual bytes the computer uses to store the data. Since we are talking about 32-bit integers, think about how such an integer is stored in memory. Now assume weâ€™ve taken exactly those bytes from memory and written them to disk. Thatâ€™s all weâ€™re talking about here. You just need to get them back off the disk and into a form you can work with.â€</p>

<p>â€œOk, soâ€¦ones and zeroes.â€</p>

<p>â€œSure, if you want to think of it that way.â€</p>

<p>They think for a while. I try to get them to relax. â€œThink out loud, let me know your thought processâ€ I say, gesturing a bit to try and get them to loosen up, â€œif youâ€™re not sure how to approach it we can just come up with some kind of sketch of an API for reading the data and I can walk you through it. The details really arenâ€™t that important.â€</p>

<p>The truth is, I donâ€™t especially care if they know the API for reading integers stored as bytes on disk. Thatâ€¦thatâ€™s not the problem. The problem is, well, weâ€™ll see in a moment.</p>

<p>The candidate is now writing some Python code to try and open a file and read data from it. They make a call to something called <code class="highlighter-rouge">readlines()</code> (Iâ€™m not a Python programmer, but I can certainly guess what that does) and then they comment:</p>

<p>â€œSo I assume each number is on a line by itself, and since you said itâ€™s binary Iâ€™m going to read each character and test if itâ€™s a one or a zero. You said these are 32 bit integers, so I guess I will need to read each one or zero 32 times. Then I need a way to convert those to a number somehowâ€¦Iâ€™m trying to think how to do that.â€</p>

<p>Iâ€™m giving them a smile of encouragement and preparing to go back over how the computer stores 32 bit integers, but mentally Iâ€™m shaking my head and thinking â€œNot again!â€</p>

<h2 id="a-worrisome-trend">A worrisome trend</h2>
<p>The interview candidate has concluded that if you take a 32-bit integer and need to store it as â€˜binary dataâ€™ in a file, you are going to have a text file that looks something like this:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>00000000000000000000000000000001
00000000000000000000000000010011
00000000110001100110111110111111 
</code></pre></div></div>

<p>â€¦where ASCII â€˜1â€™ and â€˜0â€™ characters are strung together, each number on a line, for the extent of the file.</p>

<p>Hereâ€™s the reality: Iâ€™ve had at least half-a-dozen almost identical scenarios in the past two years where a bright young prospective employee, just finishing up a college degree in computer science, appears to really struggle to think on the computerâ€™s terms: in terms of the basic â€œstuffâ€ a computer deals with: bits and bytes, memory and storage. A decade ago, and Iâ€™ve been interviewing for longer than that, I never encountered this reaction to my favorite interview question.</p>

<p>I wonâ€™t give the exact interview question I ask here (I might discuss it in some later post), but the bottom line is that itâ€™s a practical problem where I describe handing the employee-to-be a storage drive of some kind that has some files on it. I explain that the files contain simple integer sequences stored as binary data: 32-bit integers, one after the other. They will need to do some processing of these integer sequences to solve the interview question.</p>

<p>As I mentioned, I really (really!) do not care if they have no clue how to read the files. An API like <code class="highlighter-rouge">fread</code> from C or <code class="highlighter-rouge">FileStream</code> from C# or <code class="highlighter-rouge">InputStream</code> from Java all follow a simple pattern thatâ€™s easy to explain and pseudo-code into the solution without spending a lot of time on the details. That is: you have a buffer to put bytes into, you have a stream that internally has a pointer that advances as you read data, you ask for a certain number of bytes, your buffer gets filled, you do some work with the bufferâ€¦repeat until youâ€™ve read everything. Itâ€™s straight-forward.</p>

<p>The worrisome trend is the number of candidates that canâ€™t articulate or reason about the idea that a 32-bit integer is ultimately represented in memory, and on disk, as a sequence of four bytes. Their experience working with files, if to any extent at all, seems to have moved entirely to working with text data: JSON in particular seems very popular in college projects (mirroring the industry trend, I suppose) and especially seems to be the the case for those with a Machine Learning focus whichâ€¦boyâ€¦seems to be just about everyone these days.</p>

<p>Machine learning.
Python. 
Data science. 
Text files.
Canâ€™t work with â€˜binaryâ€™ data in the form of raw bytes.</p>

<p>That seems to be the trend.</p>

<h2 id="perhaps-im-in-the-wrong">Perhaps Iâ€™m in the wrong</h2>
<p>Iâ€™ve had a couple of colleagues, upon hearing my lament about â€œkids these daysâ€ or words to that effect, comment that perhaps itâ€™s time for my trusty interview question to be retired. â€œThey just donâ€™t teach that stuff any longerâ€ Iâ€™m told.</p>

<p>Are they right? If youâ€™re a smart, aspiring professional computer programmer in this day and age of zillions of web frameworks and â€˜full stackâ€™ developers and machine learning everywhere and all the latest dockerized container infrastructure blah blah blah buzzword of the week, do you need to know that low-level bits and bytes stuff?</p>

<p>Iâ€™d like to think itâ€™s still important.</p>

<p>Iâ€™m not a systems programmer. I generally donâ€™t do â€˜low levelâ€™. Almost all of my coding has been in C# and much of it working at very high levels of abstraction, interfacing with databases, talking to web services, gluing things together. Yet I like to think I have a grasp on at least the most basic of basics: like, if I need to calculate the size of a struct with two ints and a long and estimate how much data we are going to be processing pushing 10,000 of those per-second, I can do the back-of-the-envelope calculation. If I need to optimize a serialization bottleneck, I can think in terms of the right trade-offs between data size and CPU time and parsing convenience vs raw speed. That kind of thinking requires at least a fundamental understanding that when you read â€˜binary dataâ€™ you are not reading ASCII characters â€˜1â€™ and â€˜0â€™ from a text file!</p>

<p>So, if youâ€™re on one of my interview loops there are some fundamental things I just really wish you would know. We can go into a few specifics in a follow-up post.</p>
:ET